#!/usr/bin/env python3
"""
Test Brain Trust - Distributed Cognitive Architecture
Tests the new brain departments system
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from pathlib import Path

# Add project root to path
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from mind.brain_departments import brain_trust, BrainDepartment, BrainConsensus


async def test_brain_departments():
    """Test individual brain departments"""
    print("ğŸ§  Testing Brain Departments...")
    
    test_inputs = [
        "Analyze system performance and identify bottlenecks",
        "Design a new feature for user authentication",
        "Review the current codebase for security vulnerabilities",
        "Document the evolution process and create user guide",
        "Optimize memory usage in the main application"
    ]
    
    for i, test_input in enumerate(test_inputs, 1):
        print(f"\nğŸ“ Test {i}: {test_input}")
        
        try:
            # Test individual departments
            for dept in BrainDepartment:
                if dept in brain_trust.departments:
                    agent = brain_trust.departments[dept]
                    result = await agent.process_input(test_input, {"test": True})
                    
                    print(f"  [{dept.value.upper()}] Confidence: {result.confidence:.2f}")
                    print(f"       Output: {result.output[:100]}...")
                    
                    if result.confidence < 0.5:
                        print(f"       âš ï¸  Low confidence detected")
        
        except Exception as e:
            print(f"  âŒ Error testing {test_input}: {e}")


async def test_brain_pipeline():
    """Test the complete brain pipeline"""
    print("\nğŸ”„ Testing Brain Pipeline...")
    
    test_cases = [
        {
            "input": "Implement a new self-improvement algorithm",
            "context": {"priority": "high", "domain": "evolution"}
        },
        {
            "input": "Analyze and fix the current performance issues",
            "context": {"priority": "medium", "domain": "optimization"}
        },
        {
            "input": "Design a new user interface for the monitoring dashboard",
            "context": {"priority": "low", "domain": "ui"}
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ¯ Test Case {i}: {test_case['input']}")
        
        try:
            start_time = time.time()
            consensus = await brain_trust.process_through_pipeline(
                test_case["input"], 
                test_case["context"]
            )
            end_time = time.time()
            
            print(f"  â±ï¸  Processing time: {end_time - start_time:.2f}s")
            print(f"  ğŸ¯ Confidence: {consensus.confidence_score:.2f}")
            print(f"  ğŸ“Š Departments involved: {len(consensus.department_votes)}")
            print(f"  ğŸ” Conflicts detected: {len(consensus.conflicts)}")
            
            if consensus.conflicts:
                print(f"  âš ï¸  Conflicts: {consensus.conflicts}")
            
            print(f"  ğŸ’¡ Final Decision: {consensus.final_decision[:200]}...")
            print(f"  ğŸ§  Meta Analysis: {consensus.meta_analysis[:200]}...")
            
            # Show department outputs
            print("  ğŸ“‹ Department Outputs:")
            for dept, output in consensus.department_votes.items():
                print(f"    [{dept.value}]: {output[:100]}...")
        
        except Exception as e:
            print(f"  âŒ Error in pipeline test: {e}")


async def test_brain_evolution():
    """Test brain evolution capabilities"""
    print("\nğŸš€ Testing Brain Evolution...")
    
    evolution_tasks = [
        "Evolve the system to handle higher load",
        "Improve the reasoning capabilities",
        "Optimize the memory management system",
        "Enhance the security features"
    ]
    
    for i, task in enumerate(evolution_tasks, 1):
        print(f"\nğŸ”„ Evolution Task {i}: {task}")
        
        try:
            consensus = await brain_trust.process_through_pipeline(task, {
                "evolution_type": "self_improvement",
                "priority": "high"
            })
            
            print(f"  ğŸ¯ Evolution confidence: {consensus.confidence_score:.2f}")
            
            if consensus.confidence_score > 0.7:
                print(f"  âœ… High confidence evolution - ready to apply")
            elif consensus.confidence_score > 0.5:
                print(f"  âš ï¸  Medium confidence - needs review")
            else:
                print(f"  âŒ Low confidence - evolution blocked")
            
            print(f"  ğŸ’¡ Evolution plan: {consensus.final_decision[:200]}...")
            
        except Exception as e:
            print(f"  âŒ Error in evolution test: {e}")


def test_brain_status():
    """Test brain status reporting"""
    print("\nğŸ“Š Testing Brain Status...")
    
    try:
        status = brain_trust.get_department_status()
        print(f"  ğŸ“ˆ Active departments: {len(status)}")
        
        for dept_name, dept_status in status.items():
            print(f"    [{dept_name}]")
            print(f"      Model: {dept_status.get('model', 'unknown')}")
            print(f"      Memory: {dept_status.get('memory_size', 0)} entries")
            print(f"      Tools: {dept_status.get('tools_count', 0)}")
            print(f"      Status: {dept_status.get('status', 'unknown')}")
        
        history = brain_trust.get_consensus_history()
        print(f"  ğŸ“š Consensus history: {len(history)} decisions")
        
        if history:
            latest = history[-1]
            print(f"  ğŸ•’ Latest decision: {latest.get('timestamp', 'unknown')}")
            print(f"  ğŸ¯ Latest confidence: {latest.get('confidence', 0):.2f}")
        
    except Exception as e:
        print(f"  âŒ Error getting brain status: {e}")


async def main():
    """Main test function"""
    print("ğŸ§  ARK Brain Trust Test Suite")
    print("=" * 50)
    
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        # Test individual departments
        await test_brain_departments()
        
        # Test complete pipeline
        await test_brain_pipeline()
        
        # Test evolution capabilities
        await test_brain_evolution()
        
        # Test status reporting
        test_brain_status()
        
        print("\nâœ… Brain Trust tests completed!")
        
    except Exception as e:
        print(f"\nâŒ Test suite failed: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main()) 